{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-03T13:56:31.998211Z",
     "start_time": "2025-10-03T13:56:27.711512Z"
    }
   },
   "source": [
    "import os, glob, math, json, random, time\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (Input, Conv2D, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, Activation)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs:\", gpus)\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except:\n",
    "    pass\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.10.1\n",
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T13:56:32.062075Z",
     "start_time": "2025-10-03T13:56:32.046151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "class CFG:\n",
    "    # ===== ĐƯỜNG DẪN =====\n",
    "    DATA_ROOT = r\"E:\\Pycharm\\Advanced-Reading-on-Computer-Vision\\Datasets\\UCF50\\UCF50\"   # <-- đổi path của bạn\n",
    "    OUTPUT_DIR = \"./runs_2d_fusion\"\n",
    "    SEED = 1337\n",
    "\n",
    "    # ===== SAMPLING (TSN) =====\n",
    "    NUM_SEGMENTS = 4        # số đoạn TSN\n",
    "    CLIP_LEN = 4            # khung/đoạn\n",
    "    NUM_FRAMES = NUM_SEGMENTS * CLIP_LEN\n",
    "\n",
    "    IMG_SIZE = 224          # dùng backbone pretrain -> 224\n",
    "    CENTER_CROP = False\n",
    "\n",
    "    # ===== TRAIN =====\n",
    "    VAL_RATIO = 0.2\n",
    "    BATCH_SIZE = 8\n",
    "    EPOCHS = 10\n",
    "    LR = 1e-4\n",
    "    LABEL_SMOOTHING = 0.0\n",
    "    DROPOUT = 0.2\n",
    "\n",
    "    # ===== AUGMENT =====\n",
    "    HFLIP_PROB = 0.5\n",
    "    BRIGHTNESS_DELTA = 20  # 0..255\n",
    "\n",
    "    # ===== FUSION =====\n",
    "    FUSION_MODE = \"two_stream_late\"  # one of: 'rgb_only' | 'flow_only' | 'two_stream_late' | 'early_fusion'\n",
    "    LATE_FUSION_ALPHA = 0.5          # weight for RGB logits; (1-alpha) for Flow\n",
    "\n",
    "    # ===== BACKBONE =====\n",
    "    BACKBONE = \"efficientnetb0\"      # 'efficientnetb0' | 'mobilenetv2' | 'resnet50'\n",
    "    TRAIN_BACKBONE = True            # fine-tune head + backbone\n",
    "\n",
    "# seed + io\n",
    "random.seed(CFG.SEED); np.random.seed(CFG.SEED); tf.random.set_seed(CFG.SEED)\n",
    "os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n"
   ],
   "id": "b44b2f7c3e93c48",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T13:56:32.084928Z",
     "start_time": "2025-10-03T13:56:32.068800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def tsn_sample_indices(total, num_segments, clip_len):\n",
    "    if total <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    seg_len = max(total // num_segments, 1)\n",
    "    idxs = []\n",
    "    for s in range(num_segments):\n",
    "        start = s * seg_len\n",
    "        end = (s + 1) * seg_len\n",
    "        center = (start + end) // 2\n",
    "        for i in range(clip_len):\n",
    "            idxs.append(min(max(0, center + i - clip_len // 2), total - 1))\n",
    "    return np.array(sorted(set(idxs)), dtype=int)\n",
    "\n",
    "def _center_crop(img):\n",
    "    h, w = img.shape[:2]\n",
    "    side = min(h, w)\n",
    "    y0 = (h - side) // 2\n",
    "    x0 = (w - side) // 2\n",
    "    return img[y0:y0+side, x0:x0+side]\n",
    "\n",
    "def read_rgb_frames(path, idxs, img_size=224, center_crop=False):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n",
    "    if total <= 0:\n",
    "        cap.release();\n",
    "        return None\n",
    "    want = set(idxs.tolist())\n",
    "    frames = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        if i in want:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            if center_crop:\n",
    "                frame = _center_crop(frame)\n",
    "            frame = cv2.resize(frame, (img_size, img_size), interpolation=cv2.INTER_AREA)\n",
    "            frames.append(frame)\n",
    "            if len(frames) == len(idxs): break\n",
    "        i += 1\n",
    "    cap.release()\n",
    "    if len(frames) != len(idxs):\n",
    "        fill = frames[-1].copy() if frames else np.zeros((img_size,img_size,3), np.uint8)\n",
    "        while len(frames) < len(idxs):\n",
    "            frames.append(fill.copy())\n",
    "    return np.stack(frames, axis=0)  # (T,H,W,3)\n"
   ],
   "id": "4e9b096c6f08a3a2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T13:56:32.107087Z",
     "start_time": "2025-10-03T13:56:32.091003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def flow_tvl1(rgb_clip_uint8):\n",
    "    \"\"\"\n",
    "    rgb_clip_uint8: (T,H,W,3) uint8 (khung đã resize)\n",
    "    Trả về: list ảnh flow dạng 3-kênh (HxWx3) uint8 (dx, dy, magnitude) đã scale 0..255\n",
    "    \"\"\"\n",
    "    T, H, W, _ = rgb_clip_uint8.shape\n",
    "    gray = [cv2.cvtColor(rgb_clip_uint8[t], cv2.COLOR_RGB2GRAY) for t in range(T)]\n",
    "\n",
    "    # Check if DualTVL1OpticalFlow is available, otherwise use Farneback\n",
    "    try:\n",
    "        of = cv2.optflow.DualTVL1OpticalFlow_create()\n",
    "        use_tvl1 = True\n",
    "    except (AttributeError, ImportError):\n",
    "        # Silently fall back to Farneback optical flow\n",
    "        use_tvl1 = False\n",
    "\n",
    "    flow_imgs = []\n",
    "    for t in range(T-1):\n",
    "        if use_tvl1:\n",
    "            f = of.calc(gray[t], gray[t+1], None)  # (H,W,2) float32\n",
    "        else:\n",
    "            # Use Farneback optical flow as fallback\n",
    "            f = cv2.calcOpticalFlowFarneback(gray[t], gray[t+1], None,\n",
    "                                           0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        dx, dy = f[...,0], f[...,1]\n",
    "        mag = np.sqrt(dx*dx + dy*dy)\n",
    "\n",
    "        # scale -> 0..255 ổn định\n",
    "        def to_u8(x):\n",
    "            x = np.tanh(x/10.0)  # squash outliers\n",
    "            x = (x - x.min()) / (x.max() - x.min() + 1e-6)\n",
    "            return (x * 255.0).astype(np.uint8)\n",
    "        dx_u8 = to_u8(dx)\n",
    "        dy_u8 = to_u8(dy)\n",
    "        mag_u8 = to_u8(mag)\n",
    "\n",
    "        flow_rgb = np.stack([dx_u8, dy_u8, mag_u8], axis=-1)  # (H,W,3)\n",
    "        flow_imgs.append(flow_rgb)\n",
    "\n",
    "    # số ảnh flow = T-1, pad 1 ảnh cuối để khớp T\n",
    "    flow_imgs.append(flow_imgs[-1].copy())\n",
    "    return np.stack(flow_imgs, axis=0)  # (T,H,W,3)\n"
   ],
   "id": "57e6078f047a6af8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T13:56:32.212518Z",
     "start_time": "2025-10-03T13:56:32.112468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def scan_dataset(root):\n",
    "    classes = sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))])\n",
    "    class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "    samples = []\n",
    "    for c in classes:\n",
    "        vids = []\n",
    "        for ext in (\"*.avi\",\"*.mp4\",\"*.mkv\",\"*.mov\"):\n",
    "            vids.extend(glob.glob(os.path.join(root, c, ext)))\n",
    "        for v in vids:\n",
    "            samples.append({\"path\": v, \"label\": class_to_idx[c], \"cls\": c})\n",
    "    return samples, classes\n",
    "\n",
    "samples, classes = scan_dataset(CFG.DATA_ROOT)\n",
    "num_classes = len(classes)\n",
    "print(f\"Tổng video: {len(samples)} | Số lớp: {num_classes}\")\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(samples)),\n",
    "    test_size=CFG.VAL_RATIO,\n",
    "    random_state=CFG.SEED,\n",
    "    stratify=[s[\"label\"] for s in samples]\n",
    ")\n",
    "train_samples = [samples[i] for i in train_idx]\n",
    "val_samples   = [samples[i] for i in val_idx]\n",
    "print(f\"Train: {len(train_samples)} | Val: {len(val_samples)}\")\n"
   ],
   "id": "95963c85f02f4a16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng video: 6681 | Số lớp: 50\n",
      "Train: 5344 | Val: 1337\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T13:56:32.232960Z",
     "start_time": "2025-10-03T13:56:32.217281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def random_horizontal_flip(x, p=0.5):\n",
    "    if random.random() < p:\n",
    "        return np.flip(x, axis=2).copy()\n",
    "    return x\n",
    "\n",
    "def random_brightness(x, delta=20):\n",
    "    if delta <= 0: return x\n",
    "    shift = random.randint(-delta, delta)\n",
    "    y = np.clip(x.astype(np.int32) + shift, 0, 255).astype(np.uint8)\n",
    "    return y\n",
    "\n",
    "def load_clip_paths_and_targets(item):\n",
    "    path = item[\"path\"]; label = item[\"label\"]\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n",
    "    cap.release()\n",
    "    idxs = tsn_sample_indices(total, CFG.NUM_SEGMENTS, CFG.CLIP_LEN)\n",
    "    rgb_clip = read_rgb_frames(path, idxs, CFG.IMG_SIZE, CFG.CENTER_CROP)  # (T,H,W,3) uint8\n",
    "    return rgb_clip, label\n",
    "\n",
    "def preprocess_sample(item, training=True, mode=\"rgb_only\"):\n",
    "    rgb_clip, label = load_clip_paths_and_targets(item)\n",
    "    if rgb_clip is None:\n",
    "        rgb_clip = np.zeros((CFG.NUM_FRAMES, CFG.IMG_SIZE, CFG.IMG_SIZE, 3), np.uint8)\n",
    "\n",
    "    if training:\n",
    "        rgb_clip = random_horizontal_flip(rgb_clip, CFG.HFLIP_PROB)\n",
    "        rgb_clip = random_brightness(rgb_clip, CFG.BRIGHTNESS_DELTA)\n",
    "\n",
    "    if mode in [\"rgb_only\", \"two_stream_late\"]:\n",
    "        rgb_f32 = rgb_clip.astype(np.float32) / 255.0  # (T,H,W,3)\n",
    "    else:\n",
    "        rgb_f32 = None\n",
    "\n",
    "    if mode in [\"flow_only\", \"two_stream_late\", \"early_fusion\"]:\n",
    "        flow_clip = flow_tvl1(rgb_clip)                # (T,H,W,3) uint8\n",
    "        if training:\n",
    "            flow_clip = random_horizontal_flip(flow_clip, CFG.HFLIP_PROB)\n",
    "        flow_f32 = flow_clip.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        flow_f32 = None\n",
    "\n",
    "    if mode == \"early_fusion\":\n",
    "        # concat kênh -> (T,H,W,6)\n",
    "        fused = np.concatenate([rgb_clip, flow_clip], axis=-1).astype(np.float32) / 255.0\n",
    "        x = fused\n",
    "    elif mode == \"rgb_only\":\n",
    "        x = rgb_f32\n",
    "    elif mode == \"flow_only\":\n",
    "        x = flow_f32\n",
    "    elif mode == \"two_stream_late\":\n",
    "        x = (rgb_f32, flow_f32)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported mode\")\n",
    "\n",
    "    y = tf.keras.utils.to_categorical(label, num_classes)\n",
    "    return x, y\n",
    "\n",
    "def make_dataset(sample_list, batch_size, training=True, mode=\"rgb_only\"):\n",
    "    def gen():\n",
    "        for it in sample_list:\n",
    "            yield preprocess_sample(it, training=training, mode=mode)\n",
    "\n",
    "    if mode == \"two_stream_late\":\n",
    "        out_sig = (\n",
    "            (\n",
    "                tf.TensorSpec((CFG.NUM_FRAMES, CFG.IMG_SIZE, CFG.IMG_SIZE, 3), tf.float32),\n",
    "                tf.TensorSpec((CFG.NUM_FRAMES, CFG.IMG_SIZE, CFG.IMG_SIZE, 3), tf.float32),\n",
    "            ),\n",
    "            tf.TensorSpec((num_classes,), tf.float32),\n",
    "        )\n",
    "    elif mode == \"early_fusion\":\n",
    "        out_sig = (\n",
    "            tf.TensorSpec((CFG.NUM_FRAMES, CFG.IMG_SIZE, CFG.IMG_SIZE, 6), tf.float32),\n",
    "            tf.TensorSpec((num_classes,), tf.float32),\n",
    "        )\n",
    "    else:\n",
    "        out_sig = (\n",
    "            tf.TensorSpec((CFG.NUM_FRAMES, CFG.IMG_SIZE, CFG.IMG_SIZE, 3), tf.float32),\n",
    "            tf.TensorSpec((num_classes,), tf.float32),\n",
    "        )\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(gen, output_signature=out_sig)\n",
    "    if training:\n",
    "        ds = ds.shuffle(256, seed=CFG.SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n"
   ],
   "id": "a6ba6bb0dec7201",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T13:56:32.249039Z",
     "start_time": "2025-10-03T13:56:32.232960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def build_backbone(input_shape=(224,224,3), weights='imagenet', trainable=True):\n",
    "    if CFG.BACKBONE.lower() == \"efficientnetb0\":\n",
    "        base = keras.applications.EfficientNetB0(include_top=False, weights=weights, input_shape=input_shape, pooling='avg')\n",
    "    elif CFG.BACKBONE.lower() == \"mobilenetv2\":\n",
    "        base = keras.applications.MobileNetV2(include_top=False, weights=weights, input_shape=input_shape, pooling='avg')\n",
    "    elif CFG.BACKBONE.lower() == \"resnet50\":\n",
    "        base = keras.applications.ResNet50(include_top=False, weights=weights, input_shape=input_shape, pooling='avg')\n",
    "    else:\n",
    "        raise ValueError(\"Unknown backbone\")\n",
    "    base.trainable = trainable\n",
    "    return base\n",
    "\n",
    "def build_rgb_stream(num_classes):\n",
    "    # Input: (T,H,W,3) -> TimeDistributed(backbone) -> logits per frame -> mean over time\n",
    "    inp = Input(shape=(CFG.NUM_FRAMES, CFG.IMG_SIZE, CFG.IMG_SIZE, 3))\n",
    "    td = layers.TimeDistributed(build_backbone((CFG.IMG_SIZE, CFG.IMG_SIZE, 3), weights='imagenet', trainable=CFG.TRAIN_BACKBONE))(inp)\n",
    "    # td: (B,T,feat)\n",
    "    td = layers.TimeDistributed(Dropout(CFG.DROPOUT))(td)\n",
    "    logits_t = layers.TimeDistributed(Dense(num_classes))(td)  # (B,T,C)\n",
    "    logits = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(logits_t)  # temporal mean\n",
    "    out = layers.Activation('softmax')(logits)\n",
    "    return Model(inp, out, name=\"RGB_Stream\"), Model(inp, logits, name=\"RGB_Logits\")\n",
    "\n",
    "def build_flow_stream(num_classes):\n",
    "    inp = Input(shape=(CFG.NUM_FRAMES, CFG.IMG_SIZE, CFG.IMG_SIZE, 3))\n",
    "    td = layers.TimeDistributed(build_backbone((CFG.IMG_SIZE, CFG.IMG_SIZE, 3), weights=None, trainable=CFG.TRAIN_BACKBONE))(inp)\n",
    "    td = layers.TimeDistributed(Dropout(CFG.DROPOUT))(td)\n",
    "    logits_t = layers.TimeDistributed(Dense(num_classes))(td)\n",
    "    logits = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(logits_t)\n",
    "    out = layers.Activation('softmax')(logits)\n",
    "    return Model(inp, out, name=\"Flow_Stream\"), Model(inp, logits, name=\"Flow_Logits\")\n",
    "\n",
    "def build_early_fusion(num_classes):\n",
    "    \"\"\"\n",
    "    Input (T,H,W,6). Stem 1x1 Conv (->3 kênh) rồi backbone.\n",
    "    \"\"\"\n",
    "    inp = Input(shape=(CFG.NUM_FRAMES, CFG.IMG_SIZE, CFG.IMG_SIZE, 6))\n",
    "    # stem đưa về 3 kênh từng frame\n",
    "    stem = layers.TimeDistributed(Conv2D(3, kernel_size=1, padding='same', use_bias=False))(inp)\n",
    "    stem = layers.TimeDistributed(BatchNormalization())(stem)\n",
    "    stem = layers.TimeDistributed(Activation('relu'))(stem)\n",
    "\n",
    "    td = layers.TimeDistributed(build_backbone((CFG.IMG_SIZE, CFG.IMG_SIZE, 3), weights='imagenet', trainable=CFG.TRAIN_BACKBONE))(stem)\n",
    "    td = layers.TimeDistributed(Dropout(CFG.DROPOUT))(td)\n",
    "    logits_t = layers.TimeDistributed(Dense(num_classes))(td)\n",
    "    logits = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(logits_t)\n",
    "    out = layers.Activation('softmax')(logits)\n",
    "    return Model(inp, out, name=\"EarlyFusion_Stream\")\n"
   ],
   "id": "4fc86a222644086f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T13:56:40.494814Z",
     "start_time": "2025-10-03T13:56:32.251375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def build_model(num_classes, mode):\n",
    "    if mode == \"rgb_only\":\n",
    "        model, _ = build_rgb_stream(num_classes)\n",
    "        return model\n",
    "    if mode == \"flow_only\":\n",
    "        model, _ = build_flow_stream(num_classes)\n",
    "        return model\n",
    "    if mode == \"two_stream_late\":\n",
    "        # build logits models\n",
    "        _, rgb_logits = build_rgb_stream(num_classes)\n",
    "        _, flow_logits = build_flow_stream(num_classes)\n",
    "        rgb_inp  = rgb_logits.input\n",
    "        flow_inp = flow_logits.input\n",
    "        logits = CFG.LATE_FUSION_ALPHA * rgb_logits.output + (1.0 - CFG.LATE_FUSION_ALPHA) * flow_logits.output\n",
    "        out = layers.Activation('softmax')(logits)\n",
    "        return Model([rgb_inp, flow_inp], out, name=\"TwoStream_LateFusion\")\n",
    "    if mode == \"early_fusion\":\n",
    "        return build_early_fusion(num_classes)\n",
    "    raise ValueError(\"Unknown mode\")\n",
    "\n",
    "model = build_model(num_classes, CFG.FUSION_MODE)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=CFG.LR),\n",
    "    loss=CategoricalCrossentropy(label_smoothing=CFG.LABEL_SMOOTHING),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary(line_length=120, expand_nested=True)\n"
   ],
   "id": "7a6cc9fda700d102",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16705208/16705208 [==============================] - 3s 0us/step\n",
      "Model: \"TwoStream_LateFusion\"\n",
      "________________________________________________________________________________________________________________________\n",
      " Layer (type)                          Output Shape               Param #       Connected to                            \n",
      "========================================================================================================================\n",
      " input_1 (InputLayer)                  [(None, 16, 224, 224, 3)]  0             []                                      \n",
      "                                                                                                                        \n",
      " input_3 (InputLayer)                  [(None, 16, 224, 224, 3)]  0             []                                      \n",
      "                                                                                                                        \n",
      " time_distributed (TimeDistributed)    (None, 16, 1280)           4049571       ['input_1[0][0]']                       \n",
      "                                                                                                                        \n",
      " time_distributed_3 (TimeDistributed)  (None, 16, 1280)           4049571       ['input_3[0][0]']                       \n",
      "                                                                                                                        \n",
      " time_distributed_1 (TimeDistributed)  (None, 16, 1280)           0             ['time_distributed[0][0]']              \n",
      "                                                                                                                        \n",
      " time_distributed_4 (TimeDistributed)  (None, 16, 1280)           0             ['time_distributed_3[0][0]']            \n",
      "                                                                                                                        \n",
      " time_distributed_2 (TimeDistributed)  (None, 16, 50)             64050         ['time_distributed_1[0][0]']            \n",
      "                                                                                                                        \n",
      " time_distributed_5 (TimeDistributed)  (None, 16, 50)             64050         ['time_distributed_4[0][0]']            \n",
      "                                                                                                                        \n",
      " tf.math.reduce_mean (TFOpLambda)      (None, 50)                 0             ['time_distributed_2[0][0]']            \n",
      "                                                                                                                        \n",
      " tf.math.reduce_mean_1 (TFOpLambda)    (None, 50)                 0             ['time_distributed_5[0][0]']            \n",
      "                                                                                                                        \n",
      " tf.math.multiply (TFOpLambda)         (None, 50)                 0             ['tf.math.reduce_mean[0][0]']           \n",
      "                                                                                                                        \n",
      " tf.math.multiply_1 (TFOpLambda)       (None, 50)                 0             ['tf.math.reduce_mean_1[0][0]']         \n",
      "                                                                                                                        \n",
      " tf.__operators__.add (TFOpLambda)     (None, 50)                 0             ['tf.math.multiply[0][0]',              \n",
      "                                                                                 'tf.math.multiply_1[0][0]']            \n",
      "                                                                                                                        \n",
      " activation_2 (Activation)             (None, 50)                 0             ['tf.__operators__.add[0][0]']          \n",
      "                                                                                                                        \n",
      "========================================================================================================================\n",
      "Total params: 8,227,242\n",
      "Trainable params: 8,143,196\n",
      "Non-trainable params: 84,046\n",
      "________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T13:56:42.361270Z",
     "start_time": "2025-10-03T13:56:40.595280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "train_ds = make_dataset(train_samples, CFG.BATCH_SIZE, training=True, mode=CFG.FUSION_MODE)\n",
    "val_ds   = make_dataset(val_samples,   CFG.BATCH_SIZE, training=False, mode=CFG.FUSION_MODE)\n",
    "\n",
    "for xb, yb in train_ds.take(1):\n",
    "    if CFG.FUSION_MODE == \"two_stream_late\":\n",
    "        xrgb, xflow = xb\n",
    "        print(\"RGB batch:\", xrgb.shape, \"Flow batch:\", xflow.shape, \"Labels:\", yb.shape)\n",
    "    else:\n",
    "        print(\"X batch:\", xb.shape, \"Labels:\", yb.shape)\n"
   ],
   "id": "ec02e4a8804b7d15",
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} AttributeError: module 'cv2' has no attribute 'optflow'\nTraceback (most recent call last):\n\n  File \"C:\\Users\\buian\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\buian\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\buian\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\buian\\AppData\\Local\\Temp\\ipykernel_17180\\1718138285.py\", line 63, in gen\n    yield preprocess_sample(it, training=training, mode=mode)\n\n  File \"C:\\Users\\buian\\AppData\\Local\\Temp\\ipykernel_17180\\1718138285.py\", line 37, in preprocess_sample\n    flow_clip = flow_tvl1(rgb_clip)                # (T,H,W,3) uint8\n\n  File \"C:\\Users\\buian\\AppData\\Local\\Temp\\ipykernel_17180\\1326869266.py\", line 9, in flow_tvl1\n    of = cv2.optflow.DualTVL1OpticalFlow_create()\n\nAttributeError: module 'cv2' has no attribute 'optflow'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnknownError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m train_ds \u001B[38;5;241m=\u001B[39m make_dataset(train_samples, CFG\u001B[38;5;241m.\u001B[39mBATCH_SIZE, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, mode\u001B[38;5;241m=\u001B[39mCFG\u001B[38;5;241m.\u001B[39mFUSION_MODE)\n\u001B[0;32m      3\u001B[0m val_ds   \u001B[38;5;241m=\u001B[39m make_dataset(val_samples,   CFG\u001B[38;5;241m.\u001B[39mBATCH_SIZE, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, mode\u001B[38;5;241m=\u001B[39mCFG\u001B[38;5;241m.\u001B[39mFUSION_MODE)\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m xb, yb \u001B[38;5;129;01min\u001B[39;00m train_ds\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m CFG\u001B[38;5;241m.\u001B[39mFUSION_MODE \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtwo_stream_late\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m      7\u001B[0m         xrgb, xflow \u001B[38;5;241m=\u001B[39m xb\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:766\u001B[0m, in \u001B[0;36mOwnedIterator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    764\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    765\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 766\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    767\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mOutOfRangeError:\n\u001B[0;32m    768\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:749\u001B[0m, in \u001B[0;36mOwnedIterator._next_internal\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    746\u001B[0m \u001B[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001B[39;00m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;66;03m# to communicate that there is no more data to iterate over.\u001B[39;00m\n\u001B[0;32m    748\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecution_mode(context\u001B[38;5;241m.\u001B[39mSYNC):\n\u001B[1;32m--> 749\u001B[0m   ret \u001B[38;5;241m=\u001B[39m \u001B[43mgen_dataset_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator_get_next\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    750\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iterator_resource\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    751\u001B[0m \u001B[43m      \u001B[49m\u001B[43moutput_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_output_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    752\u001B[0m \u001B[43m      \u001B[49m\u001B[43moutput_shapes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_output_shapes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    754\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    755\u001B[0m     \u001B[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001B[39;00m\n\u001B[0;32m    756\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_element_spec\u001B[38;5;241m.\u001B[39m_from_compatible_tensor_list(ret)  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3016\u001B[0m, in \u001B[0;36miterator_get_next\u001B[1;34m(iterator, output_types, output_shapes, name)\u001B[0m\n\u001B[0;32m   3014\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[0;32m   3015\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m-> 3016\u001B[0m   \u001B[43m_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_from_not_ok_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3017\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_FallbackException:\n\u001B[0;32m   3018\u001B[0m   \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001B[0m, in \u001B[0;36mraise_from_not_ok_status\u001B[1;34m(e, name)\u001B[0m\n\u001B[0;32m   7207\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mraise_from_not_ok_status\u001B[39m(e, name):\n\u001B[0;32m   7208\u001B[0m   e\u001B[38;5;241m.\u001B[39mmessage \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m name: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m name \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 7209\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mUnknownError\u001B[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} AttributeError: module 'cv2' has no attribute 'optflow'\nTraceback (most recent call last):\n\n  File \"C:\\Users\\buian\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\buian\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\buian\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\buian\\AppData\\Local\\Temp\\ipykernel_17180\\1718138285.py\", line 63, in gen\n    yield preprocess_sample(it, training=training, mode=mode)\n\n  File \"C:\\Users\\buian\\AppData\\Local\\Temp\\ipykernel_17180\\1718138285.py\", line 37, in preprocess_sample\n    flow_clip = flow_tvl1(rgb_clip)                # (T,H,W,3) uint8\n\n  File \"C:\\Users\\buian\\AppData\\Local\\Temp\\ipykernel_17180\\1326869266.py\", line 9, in flow_tvl1\n    of = cv2.optflow.DualTVL1OpticalFlow_create()\n\nAttributeError: module 'cv2' has no attribute 'optflow'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%\n",
    "ckpt_path = os.path.join(CFG.OUTPUT_DIR, f\"{model.name}_best.keras\")\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=5, restore_best_weights=True, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\", factor=0.5, patience=2, verbose=1),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=CFG.EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "with open(os.path.join(CFG.OUTPUT_DIR, f\"{model.name}_history.json\"), \"w\") as f:\n",
    "    json.dump(history.history, f, indent=2)\n",
    "print(\"Best ckpt:\", ckpt_path)\n"
   ],
   "id": "1a4094f09c7d70be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%\n",
    "# Gom val vào RAM để predict một lượt\n",
    "val_X, val_y = [], []\n",
    "for it in val_samples:\n",
    "    x, y = preprocess_sample(it, training=False, mode=CFG.FUSION_MODE)\n",
    "    val_X.append(x); val_y.append(np.argmax(y))\n",
    "\n",
    "# tổ chức đầu vào theo mode\n",
    "if CFG.FUSION_MODE == \"two_stream_late\":\n",
    "    X_rgb = np.stack([xx[0] for xx in val_X], axis=0)\n",
    "    X_flow= np.stack([xx[1] for xx in val_X], axis=0)\n",
    "    inputs = [X_rgb, X_flow]\n",
    "elif CFG.FUSION_MODE == \"early_fusion\":\n",
    "    inputs = np.stack(val_X, axis=0)   # (N,T,H,W,6)\n",
    "else:\n",
    "    inputs = np.stack(val_X, axis=0)   # (N,T,H,W,3)\n",
    "\n",
    "probs = model.predict(inputs, batch_size=CFG.BATCH_SIZE, verbose=1)\n",
    "preds = np.argmax(probs, axis=1)\n",
    "val_y = np.array(val_y)\n",
    "\n",
    "acc = (preds == val_y).mean()\n",
    "bal_acc = balanced_accuracy_score(val_y, preds)\n",
    "print(f\"[{model.name}] Val Accuracy: {acc:.4f} | Balanced Acc: {bal_acc:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(val_y, preds, target_names=classes, digits=4))\n",
    "\n",
    "cm = confusion_matrix(val_y, preds)\n",
    "print(\"Confusion Matrix (top-left 10x10):\")\n",
    "size = min(10, cm.shape[0])\n",
    "print(cm[:size,:size])\n"
   ],
   "id": "77267f85a4cb6732"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_history(h):\n",
    "    plt.figure()\n",
    "    plt.plot(h['loss'], label='train_loss'); plt.plot(h['val_loss'], label='val_loss')\n",
    "    plt.title('Loss'); plt.legend(); plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(h['accuracy'], label='train_acc'); plt.plot(h['val_accuracy'], label='val_acc')\n",
    "    plt.title('Accuracy'); plt.legend(); plt.show()\n",
    "\n",
    "def plot_cm(cm, classes, normalize=False, title='Confusion matrix'):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    plt.figure(figsize=(8,6)); plt.imshow(cm, interpolation='nearest'); plt.title(title); plt.colorbar()\n",
    "    ticks = np.arange(len(classes)); plt.xticks(ticks, classes, rotation=90); plt.yticks(ticks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'; thr = cm.max()/2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), ha='center', va='center', color=\"white\" if cm[i, j] > thr else \"black\")\n",
    "    plt.ylabel('True'); plt.xlabel('Pred'); plt.tight_layout()\n",
    "\n",
    "plot_history(history.history)\n",
    "plot_cm(cm, classes, False, f'CM - {model.name}')\n",
    "plot_cm(cm, classes, True,  f'CM (norm) - {model.name}')\n"
   ],
   "id": "f648cceae1428a71"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
