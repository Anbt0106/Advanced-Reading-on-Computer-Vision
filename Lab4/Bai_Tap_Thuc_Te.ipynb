{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os, xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "\n",
    "VOC_DIR = \"E:\\\\Pycharm\\\\Advanced-Reading-on-Computer-Vision\\\\Datasets\\\\VOC\"\n",
    "ann_dir = os.path.join(VOC_DIR, \"Annotations\")\n",
    "\n",
    "cnt = Counter()\n",
    "bad = 0\n",
    "for fn in os.listdir(ann_dir):\n",
    "    if not fn.endswith(\".xml\"): continue\n",
    "    root = ET.parse(os.path.join(ann_dir, fn)).getroot()\n",
    "    nobj = 0\n",
    "    for obj in root.findall(\"object\"):\n",
    "        name = (obj.find(\"name\").text or \"\").strip()\n",
    "        if name == \"\":\n",
    "            bad += 1\n",
    "            continue\n",
    "        cnt[name] += 1\n",
    "        nobj += 1\n",
    "    if nobj == 0:\n",
    "        pass\n",
    "\n",
    "print(\"== Class frequency ==\")\n",
    "for k, v in cnt.most_common():\n",
    "    print(f\"{k:20s} : {v}\")\n",
    "print(\"\\nEmpty/invalid objects:\", bad)\n",
    "print(\"Total XML files:\", len([f for f in os.listdir(ann_dir) if f.endswith('.xml')]))\n"
   ],
   "id": "96f250b8ffcc7d10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# E:\\Pycharm\\Advanced-Reading-on-Computer-Vision\\config_label_map.py\n",
    "FINAL_CLASSES = [\n",
    "    \"person\", \"vehicle\", \"animal\", \"furniture\", \"food_drink\", \"device\", \"sports\", \"signage_decor\",\n",
    "]\n",
    "\n",
    "MERGE_TO_GROUP = {\n",
    "    # person\n",
    "    \"person\": \"person\",\n",
    "\n",
    "    # vehicle\n",
    "    \"car\": \"vehicle\", \"truck\": \"vehicle\", \"bus\": \"vehicle\", \"train\": \"vehicle\",\n",
    "    \"motorcycle\": \"vehicle\", \"bicycle\": \"vehicle\", \"boat\": \"vehicle\", \"airplane\": \"vehicle\",\n",
    "\n",
    "    # animal\n",
    "    \"dog\": \"animal\", \"cat\": \"animal\", \"bird\": \"animal\", \"horse\": \"animal\", \"sheep\": \"animal\",\n",
    "    \"cow\": \"animal\", \"elephant\": \"animal\", \"bear\": \"animal\", \"zebra\": \"animal\", \"giraffe\": \"animal\",\n",
    "\n",
    "    # furniture\n",
    "    \"chair\": \"furniture\", \"couch\": \"furniture\", \"bed\": \"furniture\",\n",
    "    \"dining table\": \"furniture\", \"toilet\": \"furniture\", \"sink\": \"furniture\",\n",
    "\n",
    "    # food & drink\n",
    "    \"banana\": \"food_drink\", \"apple\": \"food_drink\", \"sandwich\": \"food_drink\", \"orange\": \"food_drink\",\n",
    "    \"broccoli\": \"food_drink\", \"carrot\": \"food_drink\", \"donut\": \"food_drink\", \"cake\": \"food_drink\",\n",
    "    \"pizza\": \"food_drink\", \"hot dog\": \"food_drink\", \"bottle\": \"food_drink\", \"cup\": \"food_drink\",\n",
    "    \"bowl\": \"food_drink\", \"wine glass\": \"food_drink\",\n",
    "\n",
    "    # device\n",
    "    \"tv\": \"device\", \"laptop\": \"device\", \"mouse\": \"device\", \"keyboard\": \"device\", \"cell phone\": \"device\",\n",
    "    \"remote\": \"device\", \"refrigerator\": \"device\", \"microwave\": \"device\", \"oven\": \"device\",\n",
    "    \"toaster\": \"device\", \"hair drier\": \"device\",\n",
    "\n",
    "    # sports\n",
    "    \"skis\": \"sports\", \"snowboard\": \"sports\", \"sports ball\": \"sports\", \"skateboard\": \"sports\",\n",
    "    \"surfboard\": \"sports\", \"tennis racket\": \"sports\", \"frisbee\": \"sports\",\n",
    "    \"baseball bat\": \"sports\", \"baseball glove\": \"sports\", \"kite\": \"sports\",\n",
    "\n",
    "    # signage / decor / ph·ª• ki·ªán\n",
    "    \"traffic light\": \"signage_decor\", \"stop sign\": \"signage_decor\", \"parking meter\": \"signage_decor\",\n",
    "    \"handbag\": \"signage_decor\", \"backpack\": \"signage_decor\", \"umbrella\": \"signage_decor\",\n",
    "    \"tie\": \"signage_decor\", \"suitcase\": \"signage_decor\", \"book\": \"signage_decor\", \"clock\": \"signage_decor\",\n",
    "    \"vase\": \"signage_decor\", \"teddy bear\": \"signage_decor\", \"scissors\": \"signage_decor\",\n",
    "    \"toothbrush\": \"signage_decor\", \"potted plant\": \"signage_decor\", \"fire hydrant\": \"signage_decor\",\n",
    "}\n"
   ],
   "id": "4231b1dd84c8d676"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os, xml.etree.ElementTree as ET\n",
    "\n",
    "VOC_DIR = r\"E:\\Pycharm\\Advanced-Reading-on-Computer-Vision\\Datasets\\VOC\"\n",
    "ann_dir = os.path.join(VOC_DIR, \"Annotations\")\n",
    "imgsets = os.path.join(VOC_DIR, \"ImageSets\", \"Main\")\n",
    "os.makedirs(imgsets, exist_ok=True)\n",
    "\n",
    "TARGET = set(FINAL_CLASSES)\n",
    "COCO2GROUP = dict(MERGE_TO_GROUP)\n",
    "GROUP_PASS = set(TARGET)\n",
    "\n",
    "kept_ids = []\n",
    "\n",
    "\n",
    "def rewrite_xml(xml_path):\n",
    "    tree = ET.parse(xml_path);\n",
    "    root = tree.getroot()\n",
    "    objs = root.findall(\"object\")\n",
    "    new_objs = []\n",
    "    for obj in objs:\n",
    "        name_node = obj.find(\"name\")\n",
    "        if name_node is None: continue\n",
    "        name = (name_node.text or \"\").strip()\n",
    "        # map\n",
    "        if name in COCO2GROUP:\n",
    "            new_name = COCO2GROUP[name]\n",
    "        elif name in GROUP_PASS:\n",
    "            new_name = name\n",
    "        else:\n",
    "            new_name = None\n",
    "        if new_name in TARGET:\n",
    "            name_node.text = new_name\n",
    "            new_objs.append(obj)\n",
    "\n",
    "    for obj in objs: root.remove(obj)\n",
    "    for obj in new_objs: root.append(obj)\n",
    "\n",
    "    if len(new_objs) == 0:\n",
    "        return False\n",
    "    tree.write(xml_path, encoding=\"utf-8\")\n",
    "    return True\n",
    "\n",
    "\n",
    "for fn in os.listdir(ann_dir):\n",
    "    if not fn.endswith(\".xml\"): continue\n",
    "    ok = rewrite_xml(os.path.join(ann_dir, fn))\n",
    "    img_id = os.path.splitext(fn)[0]\n",
    "    if ok:\n",
    "        kept_ids.append(img_id)\n",
    "    else:\n",
    "        os.remove(os.path.join(ann_dir, fn))\n",
    "\n",
    "\n",
    "def filter_ids(txt_path, keep_set):\n",
    "    if not os.path.exists(txt_path): return\n",
    "    with open(txt_path) as f:\n",
    "        ids = [x.strip() for x in f if x.strip()]\n",
    "    ids = [i for i in ids if i in keep_set]\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for i in ids: f.write(i + \"\\n\")\n",
    "\n",
    "\n",
    "keep = set(kept_ids)\n",
    "filter_ids(os.path.join(imgsets, \"train.txt\"), keep)\n",
    "filter_ids(os.path.join(imgsets, \"val.txt\"), keep)\n",
    "\n",
    "print(f\"Target classes:\", sorted(TARGET))\n"
   ],
   "id": "f8cd848e3a9215a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hu·∫•n luy·ªán l·∫°i Faster R-CNN",
   "id": "ff214fa071b3ead7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os, xml.etree.ElementTree as ET\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "VOC_DIR = r\"E:\\Pycharm\\Advanced-Reading-on-Computer-Vision\\Datasets\\VOC\"\n",
    "\n",
    "CLS_TO_IDX = {c: i + 1 for i, c in enumerate(FINAL_CLASSES)}  # 0 = background\n",
    "NUM_CLASSES = len(FINAL_CLASSES) + 1\n",
    "\n",
    "\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, root: str, image_set=\"train\", size=800, augment=False, classes: List[str] = None):\n",
    "        self.root = root\n",
    "        self.img_dir = os.path.join(root, \"JPEGImages\")\n",
    "        self.ann_dir = os.path.join(root, \"Annotations\")\n",
    "        with open(os.path.join(root, \"ImageSets\", \"Main\", f\"{image_set}.txt\")) as f:\n",
    "            self.ids = [x.strip() for x in f if x.strip()]\n",
    "        self.size = size\n",
    "        self.augment = augment\n",
    "        self.classes = classes or FINAL_CLASSES\n",
    "        self.cls_to_idx = {c: i + 1 for i, c in enumerate(self.classes)}\n",
    "        self.normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                          std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img_id = self.ids[i]\n",
    "        img = Image.open(os.path.join(self.img_dir, f\"{img_id}.jpg\")).convert(\"RGB\")\n",
    "        w0, h0 = img.size\n",
    "        img = img.resize((self.size, self.size))\n",
    "        sx, sy = self.size / w0, self.size / h0\n",
    "\n",
    "        boxes, labels = [], []\n",
    "        root = ET.parse(os.path.join(self.ann_dir, f\"{img_id}.xml\")).getroot()\n",
    "        for obj in root.findall(\"object\"):\n",
    "            name = obj.find(\"name\").text.strip()\n",
    "            if name not in self.cls_to_idx:\n",
    "                continue\n",
    "            bb = obj.find(\"bndbox\")\n",
    "            x1 = float(bb.find(\"xmin\").text) * sx\n",
    "            y1 = float(bb.find(\"ymin\").text) * sy\n",
    "            x2 = float(bb.find(\"xmax\").text) * sx\n",
    "            y2 = float(bb.find(\"ymax\").text) * sy\n",
    "            if x2 > x1 and y2 > y1:\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "                labels.append(self.cls_to_idx[name])\n",
    "\n",
    "        x = torchvision.transforms.functional.to_tensor(img)\n",
    "        if self.augment:\n",
    "            import random\n",
    "            if random.random() < 0.5 and boxes:\n",
    "                x = torchvision.transforms.functional.hflip(x)\n",
    "                for b in boxes:\n",
    "                    x1, y1, x2, y2 = b\n",
    "                    b[0], b[2] = self.size - x2, self.size - x1\n",
    "        x = self.normalize(x)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([i]),\n",
    "        }\n",
    "        return x, target\n",
    "\n",
    "\n",
    "def collate_fn(b):\n",
    "    imgs, tgts = list(zip(*b))\n",
    "    return list(imgs), list(tgts)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_ds = VOCDataset(VOC_DIR, \"train\", size=800, augment=True, classes=FINAL_CLASSES)\n",
    "val_ds = VOCDataset(VOC_DIR, \"val\", size=800, augment=False, classes=FINAL_CLASSES)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Classes:\", FINAL_CLASSES, \"| #train:\", len(train_ds), \"| #val:\", len(val_ds))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ],
   "id": "a6fbc8f55985538c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def box_iou(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    N, M = a.shape[0], b.shape[0]\n",
    "    ious = np.zeros((N, M), dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        ax1, ay1, ax2, ay2 = a[i]\n",
    "        aarea = max(0, ax2 - ax1) * max(0, ay2 - ay1)\n",
    "        if aarea <= 0: continue\n",
    "        xx1 = np.maximum(ax1, b[:, 0]);\n",
    "        yy1 = np.maximum(ay1, b[:, 1])\n",
    "        xx2 = np.minimum(ax2, b[:, 2]);\n",
    "        yy2 = np.minimum(ay2, b[:, 3])\n",
    "        inter = np.maximum(0, xx2 - xx1) * np.maximum(0, yy2 - yy1)\n",
    "        barea = np.maximum(0, b[:, 2] - b[:, 0]) * np.maximum(0, b[:, 3] - b[:, 1])\n",
    "        union = aarea + barea - inter + 1e-8\n",
    "        ious[i] = inter / union\n",
    "    return ious\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_ap50(model, loader, iou_th=0.5, score_th=0.05, max_det=100):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    all_scores, all_tp, all_fp, npos = [], [], [], 0\n",
    "    for imgs, targets in loader:\n",
    "        imgs = [im.to(device) for im in imgs]\n",
    "        outs = model(imgs)\n",
    "        for out, tgt in zip(outs, targets):\n",
    "            gt_boxes = tgt[\"boxes\"].numpy()\n",
    "            npos += len(gt_boxes)\n",
    "\n",
    "            boxes = out[\"boxes\"].cpu().numpy()\n",
    "            scores = out[\"scores\"].cpu().numpy()\n",
    "            keep = scores >= score_th\n",
    "            boxes, scores = boxes[keep], scores[keep]\n",
    "            if len(boxes) > max_det:\n",
    "                idx = np.argsort(-scores)[:max_det]\n",
    "                boxes, scores = boxes[idx], scores[idx]\n",
    "\n",
    "            ious = box_iou(boxes, gt_boxes) if (len(boxes) and len(gt_boxes)) else np.zeros((len(boxes), len(gt_boxes)))\n",
    "            order = np.argsort(-scores)\n",
    "            boxes, scores = boxes[order], scores[order]\n",
    "            ious = ious[order]\n",
    "\n",
    "            used = np.zeros((len(gt_boxes),), dtype=bool)\n",
    "            tp = np.zeros((len(boxes),), dtype=np.float32)\n",
    "            fp = np.zeros((len(boxes),), dtype=np.float32)\n",
    "            for i in range(len(boxes)):\n",
    "                if len(gt_boxes) == 0:\n",
    "                    fp[i] = 1;\n",
    "                    continue\n",
    "                j = np.argmax(ious[i])\n",
    "                if ious[i, j] >= iou_th and not used[j]:\n",
    "                    tp[i] = 1;\n",
    "                    used[j] = True\n",
    "                else:\n",
    "                    fp[i] = 1\n",
    "\n",
    "            all_scores.extend(scores.tolist())\n",
    "            all_tp.extend(tp.tolist())\n",
    "            all_fp.extend(fp.tolist())\n",
    "\n",
    "    if not all_scores:\n",
    "        return {\"AP50\": 0.0, \"Precision\": 0.0, \"Recall\": 0.0}\n",
    "\n",
    "    order = np.argsort(-np.array(all_scores))\n",
    "    tp = np.array(all_tp)[order]\n",
    "    fp = np.array(all_fp)[order]\n",
    "    tp_cum = np.cumsum(tp)\n",
    "    fp_cum = np.cumsum(fp)\n",
    "\n",
    "    rec = tp_cum / max(npos, 1)\n",
    "    prec = tp_cum / np.maximum(tp_cum + fp_cum, 1e-8)\n",
    "\n",
    "    # VOC2007 11-pt\n",
    "    ap = 0.0\n",
    "    for t in np.linspace(0, 1, 11):\n",
    "        p = prec[rec >= t].max() if np.any(rec >= t) else 0\n",
    "        ap += p / 11.0\n",
    "\n",
    "    return {\"AP50\": float(ap), \"Precision\": float(prec[-1] if len(prec) else 0),\n",
    "            \"Recall\": float(rec[-1] if len(rec) else 0)}\n"
   ],
   "id": "282bd24c6d8db715"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# B∆∞·ªõc 3: Hu·∫•n luy·ªán l·∫°i m√¥ h√¨nh Faster R-CNN\n",
    "import torchvision\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "print(\"=== B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN M√î H√åNH ===\")\n",
    "\n",
    "# Kh·ªüi t·∫°o m√¥ h√¨nh Faster R-CNN\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(\n",
    "    weights_backbone=torchvision.models.ResNet50_Weights.IMAGENET1K_V2,\n",
    "    num_classes=NUM_CLASSES\n",
    ").to(device)\n",
    "\n",
    "# Optimizer v√† scheduler\n",
    "optimizer = SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Th√¥ng s·ªë hu·∫•n luy·ªán\n",
    "num_epochs = 5  # Gi·∫£m s·ªë epoch ƒë·ªÉ ch·∫°y nhanh h∆°n tr√™n Kaggle\n",
    "print_freq = 20  # Gi·∫£m t·ª´ 50 xu·ªëng 20 ƒë·ªÉ in th√¥ng tin th∆∞·ªùng xuy√™n h∆°n\n",
    "\n",
    "print(f\"Thi·∫øt b·ªã: {device}\")\n",
    "print(f\"S·ªë l∆∞·ª£ng classes: {NUM_CLASSES}\")\n",
    "print(f\"S·ªë epoch: {num_epochs}\")\n",
    "print(f\"S·ªë m·∫´u training: {len(train_ds)}\")\n",
    "print(f\"S·ªë m·∫´u validation: {len(val_ds)}\")\n",
    "print(f\"Classes: {FINAL_CLASSES}\")\n",
    "\n",
    "best_ap50 = 0.0  # Bi·∫øn l∆∞u tr·ªØ gi√° tr·ªã AP50 t·ªët nh·∫•t\n",
    "\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"L·∫•y th√¥ng tin memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        gpu_memory_cached = torch.cuda.memory_reserved() / 1024**3  # GB\n",
    "    else:\n",
    "        gpu_memory = gpu_memory_cached = 0\n",
    "\n",
    "    cpu_memory = psutil.Process().memory_info().rss / 1024**3  # GB\n",
    "    return gpu_memory, gpu_memory_cached, cpu_memory\n",
    "\n",
    "\n",
    "def print_detailed_step_info(step, total_steps, loss_dict, running_loss, step_count, step_time):\n",
    "    \"\"\"In th√¥ng tin chi ti·∫øt sau m·ªói step\"\"\"\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    avg_loss = running_loss / step_count\n",
    "    gpu_mem, gpu_cached, cpu_mem = get_memory_usage()\n",
    "\n",
    "    # T√°ch c√°c loss components\n",
    "    loss_components = {}\n",
    "    for key, value in loss_dict.items():\n",
    "        loss_components[key] = value.item()\n",
    "\n",
    "    print(f\"  üìä Step [{step+1:4d}/{total_steps}] - {step_time:.2f}s\")\n",
    "    print(f\"     üí∞ Total Loss: {avg_loss:.4f} | Current: {sum(loss_dict.values()).item():.4f}\")\n",
    "\n",
    "    # In chi ti·∫øt t·ª´ng loss component\n",
    "    if 'loss_classifier' in loss_components:\n",
    "        print(f\"     üéØ Classifier: {loss_components['loss_classifier']:.4f}\")\n",
    "    if 'loss_box_reg' in loss_components:\n",
    "        print(f\"     üì¶ Box Reg: {loss_components['loss_box_reg']:.4f}\")\n",
    "    if 'loss_objectness' in loss_components:\n",
    "        print(f\"     üîç Objectness: {loss_components['loss_objectness']:.4f}\")\n",
    "    if 'loss_rpn_box_reg' in loss_components:\n",
    "        print(f\"     üé™ RPN Box: {loss_components['loss_rpn_box_reg']:.4f}\")\n",
    "\n",
    "    print(f\"     üìà Learning Rate: {current_lr:.6f}\")\n",
    "    print(f\"     üíæ Memory - GPU: {gpu_mem:.2f}GB | CPU: {cpu_mem:.2f}GB\")\n",
    "    print(f\"     ‚è±Ô∏è  Steps/sec: {1/step_time:.2f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    epoch_start_time = time.time()\n",
    "    step_count = 0\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ EPOCH {epoch+1}/{num_epochs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        step_start_time = time.time()\n",
    "\n",
    "        # Chuy·ªÉn data l√™n device\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += losses.item()\n",
    "        step_count += 1\n",
    "        step_time = time.time() - step_start_time\n",
    "\n",
    "        # In th√¥ng tin chi ti·∫øt sau m·ªói step\n",
    "        if i % print_freq == 0:\n",
    "            print_detailed_step_info(i, len(train_loader), loss_dict,\n",
    "                                   running_loss, step_count, step_time)\n",
    "\n",
    "            # Clear cache ƒë·ªÉ tr√°nh memory leak\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    # C·∫≠p nh·∫≠t learning rate\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step()\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # T√≠nh to√°n th√¥ng s·ªë epoch\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    avg_epoch_loss = running_loss / len(train_loader)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìã EPOCH {epoch+1} SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"‚è∞ Th·ªùi gian: {epoch_time:.2f}s ({epoch_time/60:.1f} ph√∫t)\")\n",
    "    print(f\"üìâ Loss trung b√¨nh: {avg_epoch_loss:.4f}\")\n",
    "    print(f\"üìà Learning Rate: {old_lr:.6f} ‚Üí {new_lr:.6f}\")\n",
    "    print(f\"üî¢ T·ªïng s·ªë steps: {len(train_loader)}\")\n",
    "    print(f\"‚ö° T·ªëc ƒë·ªô: {len(train_loader)/epoch_time:.2f} steps/sec\")\n",
    "\n",
    "    # ƒê√°nh gi√° sau m·ªói epoch\n",
    "    print(f\"\\nüîç ƒêang ƒë√°nh gi√° tr√™n t·∫≠p validation...\")\n",
    "    eval_start_time = time.time()\n",
    "    metrics = evaluate_ap50(model, val_loader, iou_th=0.5, score_th=0.05, max_det=100)\n",
    "    eval_time = time.time() - eval_start_time\n",
    "\n",
    "    print(f\"‚úÖ Evaluation ho√†n th√†nh trong {eval_time:.2f}s\")\n",
    "    print(f\"üéØ Validation Metrics:\")\n",
    "    print(f\"   AP50: {metrics['AP50']:.4f}\")\n",
    "    print(f\"   Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"   Recall: {metrics['Recall']:.4f}\")\n",
    "\n",
    "    # L∆ØU CHECKPOINT SAU M·ªñI EPOCH\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'epoch': epoch + 1,\n",
    "        'classes': FINAL_CLASSES,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'loss': avg_epoch_loss,\n",
    "        'metrics': metrics,\n",
    "        'training_time': epoch_time,\n",
    "        'learning_rate': new_lr\n",
    "    }\n",
    "\n",
    "    # L∆∞u checkpoint hi·ªán t·∫°i\n",
    "    checkpoint_path = f\"ckpt_voc_merged_epoch_{epoch+1}.pth\"\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u checkpoint: {checkpoint_path}\")\n",
    "\n",
    "    # L∆∞u th√™m checkpoint t·ªët nh·∫•t (theo AP50)\n",
    "    if epoch == 0 or metrics['AP50'] > best_ap50:\n",
    "        best_ap50 = metrics['AP50']\n",
    "        best_checkpoint_path = \"ckpt_voc_merged_best.pth\"\n",
    "        torch.save(checkpoint, best_checkpoint_path)\n",
    "        print(f\"üèÜ ƒê√£ l∆∞u best checkpoint: {best_checkpoint_path} (AP50: {best_ap50:.4f})\")\n",
    "\n",
    "    total_elapsed = time.time() - total_start_time\n",
    "    print(f\"‚è±Ô∏è  T·ªïng th·ªùi gian ƒë√£ train: {total_elapsed/60:.1f} ph√∫t\")\n",
    "    print(f\"üìÅ B·∫°n c√≥ th·ªÉ d·ª´ng v√† ti·∫øp t·ª•c t·ª´ epoch {epoch+1}\")\n",
    "\n",
    "# T√≠nh t·ªïng th·ªùi gian training\n",
    "total_training_time = time.time() - total_start_time\n",
    "\n",
    "# L∆∞u m√¥ h√¨nh cu·ªëi c√πng\n",
    "final_checkpoint = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'scheduler': scheduler.state_dict(),\n",
    "    'epoch': num_epochs,\n",
    "    'classes': FINAL_CLASSES,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'total_training_time': total_training_time\n",
    "}\n",
    "\n",
    "torch.save(final_checkpoint, \"ckpt_voc_merged_finetuned.pth\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéâ HO√ÄN TH√ÄNH TRAINING\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"‚è∞ T·ªïng th·ªùi gian training: {total_training_time/60:.1f} ph√∫t\")\n",
    "print(f\"üèÜ Best AP50 ƒë·∫°t ƒë∆∞·ª£c: {best_ap50:.4f}\")\n",
    "print(f\"üíæ File checkpoint cu·ªëi c√πng: ckpt_voc_merged_finetuned.pth\")\n"
   ],
   "id": "534ae870ade7dcae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ƒê√°nh gi√° m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n",
    "print(\"=== ƒê√ÅNH GI√Å M√î H√åNH ƒê√É HU·∫§N LUY·ªÜN ===\")\n",
    "\n",
    "import torchvision\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt = torch.load(\"ckpt_voc_merged_finetuned.pth\", map_location=device)\n",
    "model_eval = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(\n",
    "    weights_backbone=torchvision.models.ResNet50_Weights.IMAGENET1K_V2,\n",
    "    num_classes=NUM_CLASSES\n",
    ").to(device)\n",
    "model_eval.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "# ƒê√°nh gi√° cu·ªëi c√πng\n",
    "final_metrics = evaluate_ap50(model_eval, val_loader, iou_th=0.5, score_th=0.05, max_det=100)\n",
    "\n",
    "print(\"K·∫æT QU·∫¢ ƒê√ÅNH GI√Å CU·ªêI C√ôNG:\")\n",
    "print(f\"AP50: {final_metrics['AP50']:.4f}\")\n",
    "print(f\"Precision: {final_metrics['Precision']:.4f}\")\n",
    "print(f\"Recall: {final_metrics['Recall']:.4f}\")\n",
    "\n",
    "final_metrics\n"
   ],
   "id": "84df4e5dc71b7e84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TI·∫æP T·ª§C TRAINING T·ª™ CHECKPOINT (n·∫øu b·ªã d·ª´ng gi·ªØa ch·ª´ng)\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def resume_training_from_checkpoint(checkpoint_path, num_additional_epochs=3):\n",
    "    \"\"\"\n",
    "    H√†m ƒë·ªÉ ti·∫øp t·ª•c training t·ª´ checkpoint ƒë√£ l∆∞u\n",
    "    \"\"\"\n",
    "    print(f\"=== TI·∫æP T·ª§C TRAINING T·ª™ {checkpoint_path} ===\")\n",
    "\n",
    "    # Ki·ªÉm tra file checkpoint c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y checkpoint: {checkpoint_path}\")\n",
    "        return\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    print(f\"üìÇ ƒê√£ load checkpoint t·ª´ epoch {checkpoint['epoch']}\")\n",
    "\n",
    "    # Kh·ªüi t·∫°o l·∫°i model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(\n",
    "        weights_backbone=torchvision.models.ResNet50_Weights.IMAGENET1K_V2,\n",
    "        num_classes=checkpoint['num_classes']\n",
    "    ).to(device)\n",
    "\n",
    "    # Load state dict\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    # Kh·ªüi t·∫°o l·∫°i optimizer v√† scheduler\n",
    "    optimizer = SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    # Load optimizer v√† scheduler state\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_ap50 = checkpoint.get('metrics', {}).get('AP50', 0.0)\n",
    "\n",
    "    print(f\"üîÑ Ti·∫øp t·ª•c t·ª´ epoch {start_epoch + 1}\")\n",
    "    print(f\"üèÜ Best AP50 hi·ªán t·∫°i: {best_ap50:.4f}\")\n",
    "\n",
    "    # Ti·∫øp t·ª•c training\n",
    "    for epoch in range(start_epoch, start_epoch + num_additional_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f\"\\n--- EPOCH {epoch+1}/{start_epoch + num_additional_epochs} ---\")\n",
    "\n",
    "        for i, (images, targets) in enumerate(train_loader):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += losses.item()\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                avg_loss = running_loss / (i + 1)\n",
    "                print(f\"  Step [{i+1}/{len(train_loader)}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        avg_epoch_loss = running_loss / len(train_loader)\n",
    "\n",
    "        print(f\"  Epoch ho√†n th√†nh trong {epoch_time:.2f}s\")\n",
    "        print(f\"  Loss trung b√¨nh: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # ƒê√°nh gi√°\n",
    "        print(\"  ƒêang ƒë√°nh gi√° tr√™n t·∫≠p validation...\")\n",
    "        metrics = evaluate_ap50(model, val_loader, iou_th=0.5, score_th=0.05, max_det=100)\n",
    "        print(f\"  Validation - AP50: {metrics['AP50']:.4f}, \"\n",
    "              f\"Precision: {metrics['Precision']:.4f}, Recall: {metrics['Recall']:.4f}\")\n",
    "\n",
    "        # L∆∞u checkpoint m·ªõi\n",
    "        new_checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'classes': checkpoint['classes'],\n",
    "            'num_classes': checkpoint['num_classes'],\n",
    "            'loss': avg_epoch_loss,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "\n",
    "        checkpoint_path_new = f\"ckpt_voc_merged_epoch_{epoch+1}.pth\"\n",
    "        torch.save(new_checkpoint, checkpoint_path_new)\n",
    "        print(f\"  ‚úÖ ƒê√£ l∆∞u checkpoint: {checkpoint_path_new}\")\n",
    "\n",
    "        # C·∫≠p nh·∫≠t best checkpoint\n",
    "        if metrics['AP50'] > best_ap50:\n",
    "            best_ap50 = metrics['AP50']\n",
    "            best_checkpoint_path = \"ckpt_voc_merged_best.pth\"\n",
    "            torch.save(new_checkpoint, best_checkpoint_path)\n",
    "            print(f\"  üèÜ ƒê√£ c·∫≠p nh·∫≠t best checkpoint: {best_checkpoint_path} (AP50: {best_ap50:.4f})\")\n",
    "\n",
    "    # L∆∞u checkpoint cu·ªëi c√πng\n",
    "    final_checkpoint_path = \"ckpt_voc_merged_finetuned.pth\"\n",
    "    torch.save(new_checkpoint, final_checkpoint_path)\n",
    "    print(f\"\\n‚úÖ Ho√†n th√†nh! L∆∞u checkpoint cu·ªëi: {final_checkpoint_path}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# V√ç D·ª§ S·ª¨ D·ª§NG:\n",
    "# N·∫øu b·∫°n mu·ªën ti·∫øp t·ª•c t·ª´ epoch 1, s·ª≠ d·ª•ng:\n",
    "# model_resumed = resume_training_from_checkpoint(\"ckpt_voc_merged_epoch_1.pth\", num_additional_epochs=4)\n",
    "\n",
    "# N·∫øu b·∫°n mu·ªën ti·∫øp t·ª•c t·ª´ epoch 2, s·ª≠ d·ª•ng:\n",
    "# model_resumed = resume_training_from_checkpoint(\"ckpt_voc_merged_epoch_2.pth\", num_additional_epochs=3)\n",
    "\n",
    "print(\"\\nüìã H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG:\")\n",
    "print(\"1. N·∫øu training b·ªã d·ª´ng ·ªü epoch 2, b·∫°n c√≥ file: ckpt_voc_merged_epoch_1.pth\")\n",
    "print(\"2. ƒê·ªÉ ti·∫øp t·ª•c, ch·∫°y:\")\n",
    "print(\"   model_resumed = resume_training_from_checkpoint('ckpt_voc_merged_epoch_1.pth', num_additional_epochs=4)\")\n",
    "print(\"3. C√°c file checkpoint s·∫Ω ƒë∆∞·ª£c l∆∞u:\")\n",
    "print(\"   - ckpt_voc_merged_epoch_X.pth (sau m·ªói epoch)\")\n",
    "print(\"   - ckpt_voc_merged_best.pth (model t·ªët nh·∫•t)\")\n",
    "print(\"   - ckpt_voc_merged_finetuned.pth (model cu·ªëi c√πng)\")\n",
    "\n"
   ],
   "id": "bd9b294e89f6775b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
